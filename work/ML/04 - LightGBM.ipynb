{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_jan = pd.read_csv('../../data/bicing/processed/months/2023_01_STATIONS.csv')\n",
    "df_feb = pd.read_csv('../../data/bicing/processed/months/2023_02_STATIONS.csv')\n",
    "df_march = pd.read_csv('../../data/bicing/processed/months/2023_03_STATIONS.csv')\n",
    "df_april = pd.read_csv('../../data/bicing/processed/months/2023_04_STATIONS.csv')\n",
    "df_may = pd.read_csv('../../data/bicing/processed/months/2023_05_STATIONS.csv')\n",
    "df_june = pd.read_csv('../../data/bicing/processed/months/2023_06_STATIONS.csv')\n",
    "df_july = pd.read_csv('../../data/bicing/processed/months/2023_07_STATIONS.csv')\n",
    "df_aug = pd.read_csv('../../data/bicing/processed/months/2023_08_STATIONS.csv')\n",
    "df_sep = pd.read_csv('../../data/bicing/processed/months/2023_09_STATIONS.csv')\n",
    "df_oct = pd.read_csv('../../data/bicing/processed/months/2023_10_STATIONS.csv')\n",
    "df_nov = pd.read_csv('../../data/bicing/processed/months/2023_11_STATIONS.csv')\n",
    "df_dec = pd.read_csv('../../data/bicing/processed/months/2023_12_STATIONS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan = df_jan[(df_jan['month'] == 1) & (df_jan['year'] == 2023)]\n",
    "df_feb = df_feb[(df_feb['month'] == 2) & (df_feb['year'] == 2023)]\n",
    "df_march = df_march[(df_march['month'] == 3) & (df_march['year'] == 2023)]\n",
    "df_april = df_april[(df_april['month'] == 4) & (df_april['year'] == 2023)]\n",
    "df_may = df_may[(df_may['month'] == 5) & (df_may['year'] == 2023)]\n",
    "df_june = df_june[(df_june['month'] == 6) & (df_june['year'] == 2023)]\n",
    "df_july = df_july[(df_july['month'] == 7) & (df_july['year'] == 2023)]\n",
    "df_aug = df_aug[(df_aug['month'] == 8) & (df_aug['year'] == 2023)]\n",
    "df_sep = df_sep[(df_sep['month'] == 9) & (df_sep['year'] == 2023)]\n",
    "df_oct = df_oct[(df_oct['month'] == 10) & (df_oct['year'] == 2023)]\n",
    "df_nov = df_nov[(df_nov['month'] == 11) & (df_nov['year'] == 2023)]\n",
    "df_dec = df_dec[(df_dec['month'] == 12) & (df_dec['year'] == 2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023 = pd.concat([df_jan, df_feb, df_march, df_april, df_may, df_june, df_july, df_aug, df_sep, df_oct, df_nov, df_dec])\n",
    "df_2023.sort_values(by=['grouped_date'], inplace=True)\n",
    "df_2023.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023['season'] = df_2023['grouped_date'].dt.month.apply(lambda x: 'Spring' if x in [3, 4, 5] else 'Summer' if x in [6, 7, 8] else 'Autumn' if x in [9, 10, 11] else 'Winter')\n",
    "df_2023.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv('../../data/weather_clean.csv')\n",
    "\n",
    "df_weather.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "df_weather['day'] = df_weather['date'].dt.day.astype(int)\n",
    "df_weather['month'] = df_weather['date'].dt.month.astype(int)\n",
    "df_weather['year'] = df_weather['date'].dt.year.astype(int)\n",
    "df_weather['hour'] = df_weather['hour'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_weather = pd.merge(df_2023, df_weather, on=['year', 'month', 'day', 'hour'], how='left')\n",
    "df_2023_weather.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info_df = pd.read_csv('../../data/bicing/processed/2024_STATION_LOCATIONS.csv', dtype={'post_code': str})\n",
    "station_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_weather_station = pd.merge(df_2023_weather, station_info_df, on='station_id', how='left')\n",
    "df_2023_weather_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_weather_station.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['station_id', 'num_bikes_available',\n",
    "                   'num_bikes_available_types.mechanical',\n",
    "                   'num_bikes_available_types.ebike', 'num_docks_available', 'status',\n",
    "                   'is_renting', 'is_returning', 'day_of_week', 'is_weekend', 'grouped_date',\n",
    "                   'docking_available', 'bikes_available', 'bikes_available.mechanical',\n",
    "                   'bikes_available.ebike', 'temperature', 'rainfall', 'lat', 'lon', 'post_code', 'altitude', 'capacity', 'year', 'month', 'day', 'hour','season'\n",
    "       'grouped_minute']\n",
    "\n",
    "df_2023_ml = df_2023_weather_station[columns_to_keep]\n",
    "df_2023_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_ml[df_2023_ml['capacity'].isnull()]['station_id'].unique()\n",
    "#Station 520 missing information\n",
    "df_2023_ml = df_2023_ml[df_2023_ml['station_id'] != 520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_ml.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_ml['grouped_date'] = pd.to_datetime(df_2023_ml['grouped_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = ['01/06/2023', '04/07/2023', '04/10/2023', '05/01/2023', '06/24/2023', '08/15/2023', '09/11/2023', '10/12/2023', '11/01/2023', '12/06/2023', '12/08/2023', '12/25/2023', '12/26/2023']\n",
    "df_2023_ml['is_holiday'] = df_2023_ml['grouped_date'].isin(holidays)\n",
    "df_2023_ml[df_2023_ml['is_holiday'] == True]['grouped_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_ml['percentage_docks_available'] = df_2023_ml['num_docks_available'] / df_2023_ml['capacity']\n",
    "df_2023_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter stations if needed\n",
    "stations_ids = df_2023_ml['station_id'].unique()\n",
    "# get 5 random stations\n",
    "import random\n",
    "filter_stations = random.sample(list(stations_ids), 5)\n",
    "#filter_stations = [57, 446]\n",
    "filter_stations = [57, 32, 31, 378, 446]\n",
    "filter_stations = [57]\n",
    "df_a = df_2023_ml[df_2023_ml['station_id'].isin(filter_stations)]\n",
    "#df_a = df_2023_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM - Binary Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "df_lgbm = df_a.copy()\n",
    "\n",
    "# Select features and target\n",
    "features = ['day_of_week', 'hour', 'grouped_minute', 'temperature', 'rainfall',\n",
    "            'capacity',  'season', 'altitude', 'station_id', 'is_weekend', 'month', 'day']\n",
    "# removed month and day\n",
    "# removed feautures that are not important disctrict, is_holiday\n",
    "target = 'docking_available'\n",
    "\n",
    "X = df_lgbm[features]\n",
    "y = df_lgbm[target]\n",
    "\n",
    "# I could remove day\n",
    "categorical_features = ['day_of_week',  'season', 'station_id',\n",
    "                        'is_weekend', 'hour', 'grouped_minute', 'month']\n",
    "numerical_features = ['temperature', 'rainfall', 'capacity', 'altitude']\n",
    "\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model_lgb = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', n_jobs=-1, learning_rate=0.05, num_leaves=31)\n",
    "\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('model', model_lgb)])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Training Time: {elapsed_time} seconds\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "# Extract the model from the pipeline\n",
    "model_rf = pipeline.named_steps['model']\n",
    "importances = model_rf.feature_importances_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "encoded_categorical_features = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "all_feature_names = numerical_features + list(encoded_categorical_features)\n",
    "\n",
    "# Create DataFrame for feature importances\n",
    "feature_importances = pd.DataFrame({'feature': all_feature_names, 'importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.barh(range(len(feature_importances)), feature_importances['importance'], align=\"center\")\n",
    "plt.yticks(range(len(feature_importances)), feature_importances['feature'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
